{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4dd9b9",
   "metadata": {},
   "source": [
    "# AWS Machine Learning Î™©Ï†ÅÎ≥Ñ Í∞ÄÏÜçÍ∏∞ ÌäúÌÜ†Î¶¨Ïñº\n",
    "## [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/)Í≥º [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)Î•º [Amazon SageMaker](https://aws.amazon.com/sagemaker/)ÏôÄ Ìï®Íªò ÏÇ¨Ïö©ÌïòÏó¨ ML ÏõåÌÅ¨Î°úÎìúÎ•º ÏµúÏ†ÅÌôîÌïòÎäî Î∞©Î≤ï ÌïôÏäµ\n",
    "## ÌååÌä∏ 3/3 - SageMakerÏôÄ [Hugging Face Optimum Neuron](https://huggingface.co/docs/optimum-neuron/index)ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Bert Î™®Îç∏ÏùÑ AWS Inferentia1Î°ú Ïª¥ÌååÏùºÌïòÍ≥† Î∞∞Ìè¨ÌïòÍ∏∞\n",
    "\n",
    "**SageMaker studio Ïª§ÎÑê: PyTorch 1.13 Python 3.9 CPU - ml.t3.medium** \n",
    "\n",
    "Ïù¥ ÌäúÌÜ†Î¶¨ÏñºÏóêÏÑúÎäî Î™®Îç∏ÏùÑ AWS InferentiaÎ°ú Ïª¥ÌååÏùºÌïú Îã§Ïùå AWS Inferentia1Î°ú Íµ¨ÎèôÎêòÎäî SageMaker Ïã§ÏãúÍ∞Ñ ÏóîÎìúÌè¨Ïù∏Ìä∏Ïóê Î∞∞Ìè¨ÌïòÎäî Î∞©Î≤ïÏùÑ Î∞∞Ïö∞Í≤å Îê©ÎãàÎã§. Î®ºÏ†Ä SageMaker ÏûëÏóÖÏùÑ ÏãúÏûëÌïòÏó¨ Î™®Îç∏ÏùÑ Ïª¥ÌååÏùºÌï©ÎãàÎã§. Ïù¥ ÏûëÏóÖÏùÄ Ìïú Î≤àÎßå ÏàòÌñâÌïòÎ©¥ Îê©ÎãàÎã§. Í∑∏ ÌõÑÏóêÎäî Î™®Îç∏ÏùÑ SageMaker ÏóîÎìúÌè¨Ïù∏Ìä∏Ïóê Î∞∞Ìè¨ÌïòÍ≥† ÎßàÏßÄÎßâÏúºÎ°ú ÏòàÏ∏° Í≤∞Í≥ºÎ•º ÏñªÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "ÏÑπÏÖò 02ÏóêÏÑúÎäî Optimum Neuron APIÏóêÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Ï∂îÏ∂úÌïòÍ≥† ÌòÑÏû¨ ÌÖåÏä§Ìä∏/ÏßÄÏõêÎêòÎäî Î™®Îç∏Ïù¥ Ìè¨Ìï®Îêú ÌÖåÏù¥Î∏îÏùÑ Î†åÎçîÎßÅÌï©ÎãàÎã§(Î™©Î°ùÏóê ÏóÜÎäî Ïú†ÏÇ¨Ìïú Î™®Îç∏ÎèÑ Ìò∏ÌôòÎê† Ïàò ÏûàÏßÄÎßå ÏßÅÏ†ë ÌôïÏù∏Ìï¥Ïïº Ìï©ÎãàÎã§). Ïù¥ ÌÖåÏù¥Î∏îÏùÄ Î∞∞Ìè¨Ìï† Ïàò ÏûàÎäî Î™®Îç∏ÏùÑ Ïù¥Ìï¥ÌïòÎäî Îç∞ Ï§ëÏöîÌï©ÎãàÎã§. Í∑∏Îü¨ÎÇò Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌï¥Ïïº ÌïòÎäî Í≤ΩÏö∞, **ÌååÌä∏ 2** ÎÖ∏Ìä∏Î∂ÅÏóêÏÑú Ïú†ÏÇ¨Ìïú ÌÖåÏù¥Î∏îÏùÑ ÌôïÏù∏ÌïòÏó¨ HF Optimum NeuronÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ AWS TrainiumÏúºÎ°ú ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌï† Ïàò ÏûàÎäî Î™®Îç∏ÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî. Ïù¥Î†áÍ≤å ÌïòÎ©¥ ÏóîÎìúÌà¨ÏóîÎìú ÏÜîÎ£®ÏÖòÏùÑ Í≥ÑÌöçÌïòÍ≥† ÏßÄÍ∏à Î∞îÎ°ú Íµ¨ÌòÑÏùÑ ÏãúÏûëÌï† Ïàò ÏûàÏäµÎãàÎã§."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9565a16",
   "metadata": {},
   "source": [
    "## 1) ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a57262",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e707c",
   "metadata": {},
   "source": [
    "## 2) ÏßÄÏõêÎêòÎäî Î™®Îç∏/ÏûëÏóÖ\n",
    "\n",
    "Ïù¥Î¶Ñ Îí§Ïóê **[TP]**Í∞Ä ÏûàÎäî Î™®Îç∏ÏùÄ ÌÖêÏÑú Î≥ëÎ†¨ Ï≤òÎ¶¨Î•º ÏßÄÏõêÌï©ÎãàÎã§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8019569f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(\"../docs/optimum_neuron_models.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4024d",
   "metadata": {},
   "source": [
    "## 3) Inferentia 1Ïóê Î∞∞Ìè¨Ìï† Î™®Îç∏ Ï§ÄÎπÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6093aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import shutil\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "training_job_name=\"\"\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "if os.path.isfile(\"training_job_name.txt\"): training_job_name = open(\"training_job_name.txt\", \"r\").read().strip()\n",
    "if len(training_job_name)==0: raise Exception(\"Please run Notebook number #2 or copy the name of the training_job you ran in the previous notebook and set training_job_name\")\n",
    "checkpoint_s3_uri=f\"s3://{bucket}/output/{training_job_name}/output/model.tar.gz\"\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")\n",
    "print(f\"Training job name: {training_job_name}\")\n",
    "print(f\"Model S3 URI: {checkpoint_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ecdc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "--extra-index-url=https://pip.repos.neuron.amazonaws.com\n",
    "neuron-cc[tensorflow]==1.22.0\n",
    "optimum[neuron]==1.20.0\n",
    "optimum-neuron==0.0.23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03919e8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1) Î™®Îç∏ Ïª¥ÌååÏùº ÌååÏùº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d354f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/compile_inf1.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import os\n",
    "os.environ['NEURON_RT_NUM_CORES'] = '1'\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import tarfile\n",
    "import logging\n",
    "import argparse\n",
    "import subprocess\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.neuron import NeuronModelForSequenceClassification\n",
    "\n",
    "## Helper functions that will be used for Inference after deploying the endpoint\n",
    "## Model and tokenizer loader\n",
    "def model_fn(model_dir, context=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.environ.get(\"MODEL_ID\", \"bert-base-uncased\"))\n",
    "    model = NeuronModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    return model,tokenizer\n",
    "\n",
    "def input_fn(input_data, content_type, context=None):\n",
    "    if content_type == 'application/json':\n",
    "        req = json.loads(input_data)\n",
    "        prompt = req.get('prompt')\n",
    "        if prompt is None or len(prompt) < 3:\n",
    "            raise(\"Invalid prompt. Provide an input like: {'prompt': 'text text text'}\")\n",
    "        return prompt\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported mime type: {content_type}. Supported: application/json\")\n",
    "\n",
    "def predict_fn(input_object, model_tokenizer, context=None):\n",
    "    try:\n",
    "        model,tokenizer = model_tokenizer\n",
    "        inputs = tokenizer(input_object, truncation=True, return_tensors=\"pt\")\n",
    "        logits = model(**inputs).logits\n",
    "        idx = logits.argmax(1, keepdim=True)\n",
    "        conf = torch.gather(logits, 1, idx)\n",
    "        return torch.cat([idx,conf], 1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.    \n",
    "    parser.add_argument(\"--task\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--dynamic_batch_size\", type=bool, default=False, action=argparse.BooleanOptionalAction)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--sequence_length\", type=int, default=1)\n",
    "    parser.add_argument(\"--is_model_compressed\", type=bool, default=False, action=argparse.BooleanOptionalAction)\n",
    "    \n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])    \n",
    "    parser.add_argument(\"--checkpoint_dir\", type=str, default=os.environ[\"SM_CHANNEL_CHECKPOINT\"])\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    # Set up logging        \n",
    "    logging.basicConfig(\n",
    "        level=logging.getLevelName(\"DEBUG\"),\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(args)\n",
    "    logger.info(f\"Checkpoint files: {os.listdir(args.checkpoint_dir)}\")\n",
    "    \n",
    "    model_path = args.checkpoint_dir\n",
    "    if args.is_model_compressed:\n",
    "        logger.info(\"Decompressing model file...\")\n",
    "        with tarfile.open(os.path.join(args.checkpoint_dir, \"model.tar.gz\"), 'r:gz') as tar:\n",
    "            tar.extractall(os.path.join(args.checkpoint_dir, \"model\"))\n",
    "        model_path = os.path.join(args.checkpoint_dir, \"model\")\n",
    "        logger.info(f\"Done! Model path: {model_path}\")\n",
    "        logger.info(f\"Model path files: {os.listdir(model_path)}\")\n",
    "\n",
    "    cmd  = \"optimum-cli export neuron --disable-validation \"\n",
    "    cmd += f\"--model {model_path} \"\n",
    "    cmd += f\"--task {args.task} \"\n",
    "    cmd += f\"--sequence_length {args.sequence_length} \"\n",
    "    cmd += f\"--batch_size {args.batch_size} \"\n",
    "    if args.dynamic_batch_size: cmd += \"--dynamic-batch-size \"\n",
    "    cmd += args.model_dir\n",
    "    logger.info(f\"Final command: {cmd}\")\n",
    "    subprocess.check_call(cmd.split(' '))\n",
    "\n",
    "    code_path = os.path.join(args.model_dir, 'code')\n",
    "    os.makedirs(code_path, exist_ok=True)\n",
    "\n",
    "    shutil.copy(__file__, os.path.join(code_path, \"inference.py\"))\n",
    "    shutil.copy('requirements.txt', os.path.join(code_path, 'requirements.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdf0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "sequence_length=512\n",
    "task=\"text-classification\"\n",
    "model_id=\"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c12a3",
   "metadata": {},
   "source": [
    "### 3.2) Î™®Îç∏ Ïª¥ÌååÏùº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"compile_inf1.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    name=name_from_base(\"inf1-compile\"),\n",
    "    sagemaker_session=sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.2xlarge',\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    # Inf1 models can be compiled on any CPU\n",
    "    # so, let's use a regular CPU PyTorch image on a C5 instance\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:1.13.1-cpu-py39-ubuntu20.04-sagemaker\",\n",
    "    volume_size = 512,\n",
    "    hyperparameters={     \n",
    "        \"task\": task,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"sequence_length\": sequence_length,\n",
    "        \"dynamic_batch_size\": True,\n",
    "        \"is_model_compressed\": True\n",
    "    }\n",
    ")\n",
    "estimator.framework_version = '1.13.1' # workround when using image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98039d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit({\"checkpoint\": checkpoint_s3_uri})\n",
    "model_data=estimator.model_data\n",
    "print(f\"Model data: {model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543e2df",
   "metadata": {},
   "source": [
    "## 4) SageMaker Ïã§ÏãúÍ∞Ñ ÏóîÎìúÌè¨Ïù∏Ìä∏ Î∞∞Ìè¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0499a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# depending on the inf1 instance you deploy the model you'll have more or less \n",
    "# accelerators. We'll ask SageMaker to launch 1 worker per core\n",
    "\n",
    "pytorch_model = PyTorchModel(    \n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference-neuron:1.13.1-neuron-py310-sdk2.18.2-ubuntu20.04\",\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    name=name_from_base('bert-spam-classifier'),\n",
    "    sagemaker_session=sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    model_server_workers=4, # 1 worker per core\n",
    "    framework_version=\"1.13.1\",\n",
    "    env = {\n",
    "        'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '3600',\n",
    "        'MODEL_ID': model_id\n",
    "    }\n",
    "    # for production it is important to define vpc_config and use a vpc_endpoint\n",
    "    #vpc_config={\n",
    "    #    'Subnets': ['<SUBNET1>', '<SUBNET2>'],\n",
    "    #    'SecurityGroupIds': ['<SECURITYGROUP1>', '<DEFAULTSECURITYGROUP>']\n",
    "    #}\n",
    ")\n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8d218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.inf1.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbf30e3",
   "metadata": {},
   "source": [
    "## 5) Í∞ÑÎã®Ìïú ÌÖåÏä§Ìä∏ Ïã§Ìñâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0407a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257e8db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "labels={0: \"not spam\", 1: \"spam\"}\n",
    "not_spam=\" Deezer.com 10,406,168 Artist DB\\n\\nWe have scraped the Deezer Artist DB, right now there are 10,406,168 listings according to Deezer.com\\n\\nPlease note in going through part of the list, it is obvious there are mistakes inside their system.\\n\\nExamples include and Artist with &amp; in its name might also be found with \"and\" but the Albums for each have different totals etc. Have no clue if there are duplicate albums etc do this error in their system. Even a comma in a name could mean the Artist shows up more than once, I saw in 1 instance that 1 Artist had 6 different ArtistIDs due to spelling errors.\\n\\nSo what is this DB, very simple, it gives you the ArtistID and the actual name of the Artist in another column. If you want to see the artist you add the baseurl to the ArtistID\\n\\nAn example is ArtistID 115 is AC/DC\\n\\n[https://www.deezer.com/us/artist/115](https://www.deezer.com/us/artist/115)\\n\\nYou do not have to use [https://www.deezer.com/us/artist/](https://www.deezer.com/us/artist/) if your first language is other than English, just see if Deezer supports your language and use that baseref\\n\\nFrench for example is [https://www.deezer.com/fr/artist/115](https://www.deezer.com/fr/artist/115)\\n\\nI am providing the DB in 3 different formats:\\n\\n \\n\\nI tried posting download links here but it seems Reddit does not like that so get them here:\\n\\n[https://pastebin\\\\[DOT\\\\]com/V3KJbgif](https://pastebin.com/V3KJbgif)\\n\\n&amp;#x200B;\\n\\n**Special thanks go to** [**/user/KoalaBear84**](https://www.reddit.com/user/KoalaBear84) **for writing the scraper.**\\n\\n&amp;#x200B;\\n\\n**Cross Posted to related Reddit Groups**\"\n",
    "spam=\"üö® ATTENTION ALL USERS! üö®\\n\\nüÜò Are you looking for a way to GET RICH QUICK? üÜò\\n\\nüí∞ Don't waste your time with boring old jobs! üí∞\\n\\nüí∏ Join our CRAZY MONEY-MAKING SYSTEM today! üí∏\\n\\nü§ë Just sign up and start earning BIG BUCKS right away! ü§ë\\n\\nüëâ Plus, if you refer your friends, you'll get even MORE CASH! üëà\\n\\nüî• This is the HOTTEST OFFER of the year! üî•\\n\\nüëç Don't wait\"\n",
    "for i,text in enumerate([not_spam, spam]):\n",
    "    t=time.time()\n",
    "    pred = predictor.predict({\"prompt\": text})\n",
    "    elapsed = (time.time()-t)*1000\n",
    "    print(f\"Elapsed time: {elapsed}\")\n",
    "    print(f\"Pred: {i} - {labels[pred[0][0]]} / score: {pred[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fefdbeb",
   "metadata": {},
   "source": [
    "## 6) ÏóîÎìúÌè¨Ïù∏Ìä∏ ÏÇ≠Ï†ú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4831e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
